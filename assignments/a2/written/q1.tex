\titledquestion{Understanding word2vec}[15]
Recall that the key insight behind {\tt word2vec} is that \textit{`a word is known by the company it keeps'}. Concretely, consider a `center' word $c$ surrounded before and after by a context of a certain length. We term words in this contextual window `outside words' ($O$). For example, in Figure~\ref{fig:word2vec}, the context window length is 2, the center word $c$ is `banking', and the outside words are `turning', `into', `crises', and `as':

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{word2vec.png}
    \caption{The word2vec skip-gram prediction model with window size 2}
    \label{fig:word2vec}
\end{figure}

Skip-gram {\tt word2vec} aims to learn the probability distribution $P(O|C)$. 
Specifically, given a specific word $o$ and a specific word $c$, we want to predict $P(O=o|C=c)$: the probability that word $o$ is an `outside' word for $c$ (i.e., that it falls within the contextual window of $c$).
We model this probability by taking the softmax function over a series of vector dot-products: % I added the word "softmax" here because I bet a lot of students will have forgotten what softmax is and why the loss fn is called naive softmax. but if this is too wordy we can just take it out

\begin{equation}
 P(O=o \mid C=c) = \frac{\exp(\bu_{o}^\top \bv_c)}{\sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c)}
 \label{word2vec_condprob}
\end{equation}

For each word, we learn vectors $u$ and $v$, where $\bu_o$ is the `outside' vector representing outside word $o$, and $\bv_c$ is the `center' vector representing center word $c$. 
We store these parameters in two matrices, $\bU$ and $\bV$.
The columns of $\bU$ are all the `outside' vectors $\bu_{w}$;
the columns of $\bV$ are all of the `center' vectors $\bv_{w}$. 
Both $\bU$ and $\bV$ contain a vector for every $w \in \text{Vocabulary}$.\footnote{Assume that every word in our vocabulary is matched to an integer number $k$. Bolded lowercase letters represent vectors. $\bu_{k}$ is both the $k^{th}$ column of $\bU$ and the `outside' word vector for the word indexed by $k$. $\bv_k$ is both the $k^{th}$ column of $\bV$ and the `center' word vector for the word indexed by $k$. \textbf{In order to simplify notation we shall interchangeably use $k$ to refer to word $k$ and the index of word $k$.}}\newline

%We can think of the probability distribution $P(O|C)$ as a prediction function that we can approximate via supervised learning. For any training example, we will have a single $o$ and $c$. We will then compute a value $P(O=o|C=c)$ and report the loss. 
Recall from lectures that, for a single pair of words $c$ and $o$, the loss is given by:

\begin{equation} 
\bJ_{\text{naive-softmax}}(\bv_c, o, \bU) = -\log P(O=o| C=c).
\label{naive-softmax}
\end{equation}

We can view this loss as the cross-entropy\footnote{The \textbf{cross-entropy loss} between the true (discrete) probability distribution $p$ and another distribution $q$ is $-\sum_i p_i \log(q_i)$.} between the true distribution $\by$ and the predicted distribution $\hat{\by}$, for a particular center word c and a particular outside word o. 
Here, both $\by$ and $\hat{\by}$ are vectors with length equal to the number of words in the vocabulary.
Furthermore, the $k^{th}$ entry in these vectors indicates the conditional probability of the $k^{th}$ word being an `outside word' for the given $c$. 
The true empirical distribution $\by$ is a one-hot vector with a 1 for the true outside word $o$, and 0 everywhere else, for this particular example of center word c and outside word o.\footnote{Note that the true conditional probability distribution of context words for the entire training dataset would not be one-hot.}
The predicted distribution $\hat{\by}$ is the probability distribution $P(O|C=c)$ given by our model in equation (\ref{word2vec_condprob}). \newline

\textbf{Note:} Throughout this homework, when computing derivatives, please use the method reviewed during the lecture (i.e. no Taylor Series Approximations).

\clearpage
\begin{parts}
\part[2]
Prove that the naive-softmax loss (Equation \ref{naive-softmax}) is the same as the cross-entropy loss between $\by$  and $\hat{\by}$, i.e. (note that $\by$ 
 (true distribution), $\hat{\by}$ (predicted distribution) are vectors and $\hat{\by}_o$ is a scalar):

\begin{equation} 
-\sum_{w \in \text{Vocab}} \by_w \log(\hat{\by}_w) = - \log (\hat{\by}_o).
\end{equation}

Your answer should be one line. You may describe your answer in words.

% Ans of Q1 (a)
\ifans{
    \newline
    The true empirical distribution of $\by$ is a \textbf{one-hot} vector with a 1 for the true outside word o, and 0 everywhere else. Hence, the naive-softmax loss can be represented as follows:
    $$
        -\sum_{w \in \text{Vocab}} \by_w \log(\hat{\by}_w) 
        = \sum_{w \ne o} -0 \cdot \log(\hat{\by}_w) + \sum_{w = o} -1 \cdot \log(\hat{\by}_w)
        = -log (\hat{\by}_o)
    $$
}


% Question 1-B
\part[6]
\begin{enumerate}[label=\roman*.]
    \item 
    Compute the partial derivative of $\bJ_{\text{naive-softmax}}(\bv_c, o, \bU)$ with respect to $\bv_c$. \emph{Please write your answer in terms of $\by$, $\hat{\by}$, $\bU$, and show your work to receive full credit}.
    \begin{itemize} 
        \item \textbf{Note}: Your final answers for the partial derivative should follow the shape convention: the partial derivative of any function $f(x)$ with respect to $x$ should have the \textbf{same shape} as $x$.\footnote{This allows us to efficiently minimize a function using gradient descent without worrying about reshaping or dimension mismatching. While following the shape convention, we're guaranteed that $\theta:= \theta - \alpha\frac{\partial J(\theta)}{\partial \theta}$ is a well-defined update rule.}
        \item Please provide your answers for the partial derivative in vectorized form. For example, when we ask you to write your answers in terms of $\by$, $\hat{\by}$, and $\bU$, you may not refer to specific elements of these terms in your final answer (such as $\by_1$, $\by_2$, $\dots$). 
    \end{itemize}
    \item
    When is the gradient you computed equal to zero? \\
    \textbf{Hint:} You may wish to review and use some introductory linear algebra concepts.
    \item
    The gradient you found is the difference between the two terms. Provide an interpretation of how each of these terms improves the word vector when this gradient is subtracted from the word vector $v_c$.
\end{enumerate}

% Ans of Q1 (a)
\ifans{
    \newline
    \begin{enumerate}[label=\roman*.]

    % i.
    \item 
    \begin{align}
    \frac{\partial}{\partial \bv_c} \bJ_{\text{naive-softmax}}(\bv_c, o, \bU)
    &= \frac{\partial}{\partial \bv_c} [-\log(\exp(\bu_{o}^\top \bv_c)) + \log(\sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c))] \\
    &= -\bu_o + \frac{\partial}{\partial \bv_c} \log(\sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c)) \\
    &= -\bu_o + \frac{1}{\sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c)} \frac{\partial}{\partial \bv_c} \sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c) \\
    &= -\bu_o + \frac{1}{\sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c)} \sum_{w' \in \text{Vocab}} \exp(\bu_{w'}^\top \bv_c)\bu_{w'} \\
    &= -\bu_o + \sum_{w' \in \text{Vocab}} \frac{\exp(\bu_{w'}^\top \bv_c)\bu_{w'}}{\sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c)} \bu_{w'} \\
    &= -\bu_o + \sum_{w' \in \text{Vocab}} \hat{\by}_{w'} \bu_{w'} \\
    &= -\bu_o + \bU\hat{\by} \\
    &= -\bU\by + \bU\hat{\by} \\
    &= \bU(\hat{\by} - \by)
    \end{align}

    % ii.
    \item
    The gradient is equal to zero when $(\hat{\by} - \by)$ lies in the null space of the matrix $\bU$ which stores the outside word vectors as columns. For a matrix $\bU \in \mathbb{R}^{d \times |V|}$, where the embedding dimension $d$ is much smaller than the vocabulary size $|V|$, the columns are linearly dependent, so there exist solutions with $\hat{\by} \ne \by$.
    
    % iii.
    \item
    The gradient descent step can be defined as follows:
    $$
        \bv_c^{\text{new}} \gets \bv_c^{\text{old}} - \eta \bU\hat{\by} + \eta \bU\by
    $$
    , where $\eta$ is the step size. Intuitively speaking, adding the vector $\eta \bU\by$ to $\bv_c^{\text{old}}$ will make $\bv_c$ move towards the direction of $\bu_o$ of the outside word $o$ which lies in the context window of the center word $c$. On the other hand, subtracting the vector $\eta \bU\hat{\by}$ will make $\bv_c$ move away from all the other vectors in $\bU$. Note that all probabilities follow $0 < \hat{\by}_i < 1$.

    \end{enumerate}
}

% Question 1-C
\part[1]
In many downstream applications using word embeddings, L2 normalized vectors (e.g. $\mathbf{u}/||\mathbf{u}||_2$ where $||\mathbf{u}||_2 = \sqrt{\sum_i u_i^2}$) are used instead of their raw forms (e.g. $\mathbf{u}$). Letâ€™s consider a hypothetical downstream task of binary classification of phrases as being positive or negative, where you decide the sign based on the sum of individual embeddings of the words. When would L2 normalization take away useful information for the downstream task? When would it not?

\textbf{Hint:} Consider the case where $\mathbf{u}_x = \alpha\mathbf{u}_y$ for some words $x \neq y$ and some scalar $\alpha$. When $\alpha$ is positive, what will be the value of normalized  $\mathbf{u}_x$ and normalized $\mathbf{u}_y$? How might $\mathbf{u}_x$ and $\mathbf{u}_y$ be related for such a normalization to affect or not affect the resulting classification?

% Ans of Q1 (c)
\ifans{
    \newline
    First of all, word embeddings are vectors fully described by magnitude and direction. Suppose there exists a word embedding $\bv = \alpha \bu$, where $\alpha$ is a scaler. The normalized form of $\bu$ is $\frac{\bu}{\| \bu \|}$ and $\frac{\alpha \bu}{|\alpha| \| \bu \|}$ for $\bv$. As can be observed, the magnitude information of $\bv$ is thrown away after L2 normalization is done. For a task that takes magnitude (e.g., binary classification based on the sum of embeddings) into consideration, L2 normalization will affect the result (e.g., the sum leading to change of sign).
}

% Question 1-D
\part[5] 
Compute the partial derivatives of $\bJ_{\text{naive-softmax}}(\bv_c, o, \bU)$ with respect to each of the `outside' word vectors, $\bu_w$'s. There will be two cases: when $w=o$, the true `outside' word vector, and $w \neq o$, for all other words. Please write your answer in terms of $\by$, $\hat{\by}$, and $\bv_c$. In this subpart, you may use specific elements within these terms as well (such as $\by_1$, $\by_2$, $\dots$). Note that $\bu_w$ is a vector while $\by_1, \by_2, \dots$ are scalars. Show your work to receive full credit.

% Ans of Q1 (d)
\ifans{
    \newline
    \begin{align}
    \frac{\partial}{\partial \bu_w} \bJ_{\text{naive-softmax}}(\bv_c, o, \bU)
    &= \frac{\partial}{\partial \bu_w} [-\log(\exp(\bu_{o}^\top \bv_c)) + \log(\sum_{w \in \text{Vocab}} \exp(\bu_{w}^\top \bv_c))] \\
    &= \frac{\partial}{\partial \bu_w} (-\bu_{o}^\top \bv_c) + \frac{\partial}{\partial \bu_w} \log \sum_{w \in \text{Vocab}} \exp(\bu_w^\top \bv_c) \\
    &= \frac{\partial}{\partial \bu_w} (-\bu_{o}^\top \bv_c) + \frac{1}{\sum_{w' \in \text{Vocab}} \exp(\bu_{w'}^\top \bv_c)} \frac{\partial}{\partial \bu_w} \exp(\bu_w^\top \bv_c) \\
    &= \frac{\partial}{\partial \bu_w} (-\bu_{o}^\top \bv_c) + \frac{\exp(\bu_w^\top \bv_c)}{\sum_{w' \in \text{Vocab}} \exp(\bu_{w'}^\top \bv_c)} \bv_c \\
    &= \frac{\partial}{\partial \bu_w} (-\bu_{o}^\top \bv_c) + \hat{\by}_w \bv_c \\
    &= \begin{cases}
        (\hat{\by}_w - 1) \bv_c & \text{, if } w = o \\
        \hat{\by}_w \bv_c & \text{, otherwise}
    \end{cases}
    \end{align}
}

% Question 1-E
\part[1]
Write down the partial derivative of $\bJ_{\text{naive-softmax}}(\bv_c, o, \bU)$ with respect to $\bU$. Please break down your answer in terms of the column vectors $\frac{\partial \bJ(\bv_c, o, \bU)}{\partial \bu_1}$, $\frac{\partial \bJ(\bv_c, o, \bU)}{\partial \bu_2}$, $\cdots$, $\frac{\partial \bJ(\bv_c, o, \bU)}{\partial \bu_{|\text{Vocab}|}}$. No derivations are necessary, just an answer in the form of a matrix.

% Ans of Q1 (e)
\ifans{
    \newline
    Following the answer in (d), we can define $\frac{\partial \bJ_{\text{naive-softmax}}(\bv_c, o, \bU)}{\partial \bU} \in \mathbb{R}^{d \times |\text{Vocab}|}$ as follows:
    \begin{align}
    \frac{\partial \bJ(\bv_c, o, \bU)}{\partial \bU}
    &= \begin{bmatrix}
    \hat{\by}_1 \bv_c & \hat{\by}_2 \bv_c & ... & (\hat{\by}_o - 1) \bv_c & ... & \hat{\by}_{|\text{Vocab}|} \bv_c
    \end{bmatrix} \\ 
    &= \bv_c (\hat{\by} - \by)^\top
    \end{align}
}

\end{parts}